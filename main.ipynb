{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING SENTENCES TO LIBRARY\n",
    "import json\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "#declare trainData as an empty list\n",
    "trainData = []\n",
    "trainLabels = []\n",
    "\n",
    "nonfunctional = 0\n",
    "functional = 0\n",
    "    \n",
    "#temp list for each sentence\n",
    "tempList = []\n",
    "\n",
    "with open(\"Consolidated_data.txt\",\"r\") as f:\n",
    "    \n",
    "    \n",
    "    #read data-file line by line\n",
    "    with open(\"library.json\", \"r\") as l:\n",
    "        data = json.load(l)\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            #find where class begins\n",
    "            front = line.find(\"\\\"class\\\":\\\"\")\n",
    "            #find where class ends (where sentence begins)\n",
    "            end = line.find(\"\\\",\\\"sentence\\\":\\\"\")\n",
    "            #substring line based on front and end above\n",
    "            reqClass = (line[(front+9):end]).lower()\n",
    "\n",
    "            #sentence\n",
    "            del tempList[:]\n",
    "            temp = line.find(\"\\\"sentence\\\":\\\"\")\n",
    "            sentence = (line[(temp+12):-4]).lower()\n",
    "            sentence = sentence.translate(None, '.,-\\\":;~!@#$%^&?[]{}<>`1234567890\\\\*()').strip()\n",
    "\n",
    "            if \"nonfunctional\" in reqClass and (nonfunctional < 1796):\n",
    "                nonfunctional +=1\n",
    "                trainLabels.append(0)\n",
    "                for x in sentence.split(' '):\n",
    "                    if x in data:\n",
    "                        tempList.append(data[x])\n",
    "                    else:\n",
    "                        tempList.append(1)\n",
    "                trainData.append(tempList[:])\n",
    "            elif \"functional\" in reqClass and (functional < 1796):\n",
    "                functional +=1\n",
    "                trainLabels.append(1)\n",
    "                for x in sentence.split(' '):\n",
    "                    if x in data:\n",
    "                        tempList.append(data[x])\n",
    "                    else:\n",
    "                        tempList.append(1)\n",
    "                trainData.append(tempList[:])\n",
    "            else:\n",
    "                if (nonfunctional < 1796):\n",
    "                    nonfunctional +=1\n",
    "                    trainLabels.append(0)\n",
    "                    for x in sentence.split(' '):\n",
    "                        if x in data:\n",
    "                            tempList.append(data[x])\n",
    "                        #UNCOMMENTING IS STRONGLY NOT RECOMMENDED\n",
    "                        #else:\n",
    "                            #tempList.append(1)\n",
    "                    trainData.append(tempList[:])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabels = shuffle(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "testData = []\n",
    "testLabels = []\n",
    "\n",
    "nonfunctional = 0\n",
    "functional = 0\n",
    "\n",
    "testDataNumber = 100\n",
    "\n",
    "for x in trainLabels:\n",
    "    if x == 1 and functional < testDataNumber:\n",
    "        functional += 1\n",
    "        testData.append(trainData.pop(x))\n",
    "        testLabels.append(trainLabels.pop(x))\n",
    "    else:\n",
    "        if nonfunctional < testDataNumber:\n",
    "            nonfunctional += 1\n",
    "            testData.append(trainData.pop(x))\n",
    "            testLabels.append(trainLabels.pop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1695\n",
      "1697\n",
      "101\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels.count(1))\n",
    "print(trainLabels.count(0))\n",
    "print(testLabels.count(1))\n",
    "print(testLabels.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomizing data in two arrays without disrupting the order\n",
    "testData, testLabels = shuffle(testData, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 252, 62, 6, 676, 104]\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "#those should NOT be the same\n",
    "#print testData[0]\n",
    "print trainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = json.load(open('library.json', 'r'))\n",
    "#word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<UNK>\"] = 1 # unknown\n",
    "word_index[\"<START>\"] = 2\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'emergency contact name and phone number'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(trainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = keras.preprocessing.sequence.pad_sequences(trainData,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=32)\n",
    "\n",
    "testData = keras.preprocessing.sequence.pad_sequences(testData,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          80000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 80,289\n",
      "Trainable params: 80,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = trainData[2500:]\n",
    "partial_x_train = trainData[:2500]\n",
    "\n",
    "y_val = trainLabels[2500:]\n",
    "partial_y_train = trainLabels[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 988us/step\n",
      "[0.6937040209770202, 0.48]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 892 samples\n",
      "Epoch 1/50\n",
      "2500/2500 [==============================] - 1s 236us/step - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.4765\n",
      "Epoch 2/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6926 - acc: 0.5676 - val_loss: 0.6928 - val_acc: 0.5482\n",
      "Epoch 3/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.6920 - acc: 0.6244 - val_loss: 0.6923 - val_acc: 0.5964\n",
      "Epoch 4/50\n",
      "2500/2500 [==============================] - 0s 23us/step - loss: 0.6910 - acc: 0.6888 - val_loss: 0.6915 - val_acc: 0.6256\n",
      "Epoch 5/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.6896 - acc: 0.6988 - val_loss: 0.6904 - val_acc: 0.6379\n",
      "Epoch 6/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.6880 - acc: 0.6940 - val_loss: 0.6890 - val_acc: 0.6278\n",
      "Epoch 7/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6860 - acc: 0.6768 - val_loss: 0.6874 - val_acc: 0.6211\n",
      "Epoch 8/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6839 - acc: 0.6568 - val_loss: 0.6856 - val_acc: 0.6177\n",
      "Epoch 9/50\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 0.6814 - acc: 0.6612 - val_loss: 0.6838 - val_acc: 0.6233\n",
      "Epoch 10/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.6786 - acc: 0.6676 - val_loss: 0.6817 - val_acc: 0.6256\n",
      "Epoch 11/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6755 - acc: 0.6764 - val_loss: 0.6794 - val_acc: 0.6267\n",
      "Epoch 12/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.6719 - acc: 0.6816 - val_loss: 0.6768 - val_acc: 0.6357\n",
      "Epoch 13/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6679 - acc: 0.7000 - val_loss: 0.6741 - val_acc: 0.6413\n",
      "Epoch 14/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.6635 - acc: 0.7116 - val_loss: 0.6712 - val_acc: 0.6525\n",
      "Epoch 15/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.6587 - acc: 0.7248 - val_loss: 0.6681 - val_acc: 0.6558\n",
      "Epoch 16/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.6533 - acc: 0.7340 - val_loss: 0.6647 - val_acc: 0.6570\n",
      "Epoch 17/50\n",
      "2500/2500 [==============================] - 0s 22us/step - loss: 0.6477 - acc: 0.7368 - val_loss: 0.6612 - val_acc: 0.6502\n",
      "Epoch 18/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.6417 - acc: 0.7468 - val_loss: 0.6576 - val_acc: 0.6603\n",
      "Epoch 19/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.6351 - acc: 0.7548 - val_loss: 0.6537 - val_acc: 0.6570\n",
      "Epoch 20/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.6283 - acc: 0.7604 - val_loss: 0.6497 - val_acc: 0.6603\n",
      "Epoch 21/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.6211 - acc: 0.7628 - val_loss: 0.6456 - val_acc: 0.6592\n",
      "Epoch 22/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6135 - acc: 0.7684 - val_loss: 0.6414 - val_acc: 0.6603\n",
      "Epoch 23/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.6059 - acc: 0.7668 - val_loss: 0.6373 - val_acc: 0.6558\n",
      "Epoch 24/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.5977 - acc: 0.7716 - val_loss: 0.6332 - val_acc: 0.6581\n",
      "Epoch 25/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.5895 - acc: 0.7760 - val_loss: 0.6290 - val_acc: 0.6603\n",
      "Epoch 26/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.5812 - acc: 0.7760 - val_loss: 0.6249 - val_acc: 0.6659\n",
      "Epoch 27/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.5725 - acc: 0.7812 - val_loss: 0.6209 - val_acc: 0.6682\n",
      "Epoch 28/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.5640 - acc: 0.7820 - val_loss: 0.6171 - val_acc: 0.6637\n",
      "Epoch 29/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.5554 - acc: 0.7832 - val_loss: 0.6133 - val_acc: 0.6626\n",
      "Epoch 30/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.5467 - acc: 0.7860 - val_loss: 0.6099 - val_acc: 0.6659\n",
      "Epoch 31/50\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 0.5380 - acc: 0.7868 - val_loss: 0.6060 - val_acc: 0.6637\n",
      "Epoch 32/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.5294 - acc: 0.7892 - val_loss: 0.6029 - val_acc: 0.6670\n",
      "Epoch 33/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.5210 - acc: 0.7904 - val_loss: 0.5997 - val_acc: 0.6726\n",
      "Epoch 34/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.5124 - acc: 0.7932 - val_loss: 0.5971 - val_acc: 0.6715\n",
      "Epoch 35/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.5040 - acc: 0.7964 - val_loss: 0.5946 - val_acc: 0.6704\n",
      "Epoch 36/50\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 0.4959 - acc: 0.7996 - val_loss: 0.5919 - val_acc: 0.6682\n",
      "Epoch 37/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.4878 - acc: 0.8032 - val_loss: 0.5897 - val_acc: 0.6726\n",
      "Epoch 38/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.4798 - acc: 0.8048 - val_loss: 0.5881 - val_acc: 0.6715\n",
      "Epoch 39/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.4721 - acc: 0.8072 - val_loss: 0.5863 - val_acc: 0.6715\n",
      "Epoch 40/50\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 0.4644 - acc: 0.8116 - val_loss: 0.5850 - val_acc: 0.6783\n",
      "Epoch 41/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.4571 - acc: 0.8152 - val_loss: 0.5828 - val_acc: 0.6805\n",
      "Epoch 42/50\n",
      "2500/2500 [==============================] - 0s 23us/step - loss: 0.4498 - acc: 0.8192 - val_loss: 0.5819 - val_acc: 0.6827\n",
      "Epoch 43/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4426 - acc: 0.8204 - val_loss: 0.5817 - val_acc: 0.6839\n",
      "Epoch 44/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4356 - acc: 0.8204 - val_loss: 0.5810 - val_acc: 0.6827\n",
      "Epoch 45/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4290 - acc: 0.8260 - val_loss: 0.5797 - val_acc: 0.6872\n",
      "Epoch 46/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4223 - acc: 0.8276 - val_loss: 0.5791 - val_acc: 0.6883\n",
      "Epoch 47/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4160 - acc: 0.8308 - val_loss: 0.5786 - val_acc: 0.6872\n",
      "Epoch 48/50\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 0.4096 - acc: 0.8360 - val_loss: 0.5796 - val_acc: 0.6861\n",
      "Epoch 49/50\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 0.4038 - acc: 0.8364 - val_loss: 0.5789 - val_acc: 0.6906\n",
      "Epoch 50/50\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 0.3976 - acc: 0.8408 - val_loss: 0.5800 - val_acc: 0.6917\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 74us/step\n",
      "[0.6001617813110351, 0.66]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(testData, testLabels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
